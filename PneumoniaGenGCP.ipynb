{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47a10614",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dcfa817",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import spectral_norm\n",
    "from torchvision import models, transforms\n",
    "import torchvision\n",
    "from typing import Tuple, List, Optional, Dict, Any\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.metrics import structural_similarity as sk_ssim\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a36761d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Optional import: torchxrayvision (preferred for chest-xray pretrained models)\n",
    "try:\n",
    "    import torchxrayvision as xrv\n",
    "    _HAS_TXRV = True\n",
    "except Exception:\n",
    "    _HAS_TXRV = False\n",
    "    warnings.warn(\"torchxrayvision not available — falling back to VGG16 for perceptual features.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcc793fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cvae_unet_gan_perceptual.py\n",
    "# Full architecture + training/eval helpers for conditional VAE (U-Net) + PatchGAN + Perceptual Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8c60d37",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------\n",
    "# Utility: basic conv / upconv blocks\n",
    "# --------------------------------------\n",
    "def conv_block(in_ch, out_ch, kernel=3, stride=1, padding=1, use_bn=True):\n",
    "    layers = [nn.Conv2d(in_ch, out_ch, kernel, stride, padding, bias=not use_bn)]\n",
    "    if use_bn:\n",
    "        layers.append(nn.InstanceNorm2d(out_ch, affine=True))\n",
    "    layers.append(nn.ReLU(inplace=True))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "def upconv_block(in_ch, out_ch, kernel=3, stride=1, padding=1, use_bn=True):\n",
    "    # simple upsample + conv\n",
    "    layers = [nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "              nn.Conv2d(in_ch, out_ch, kernel, stride, padding, bias=not use_bn)]\n",
    "    if use_bn:\n",
    "        layers.append(nn.InstanceNorm2d(out_ch, affine=True))\n",
    "    layers.append(nn.ReLU(inplace=True))\n",
    "    return nn.Sequential(*layers)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10f579c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------\n",
    "# FiLM modulation layer (simple)\n",
    "# Given an embedding for label, produce scale & shift per channel\n",
    "# --------------------------------------\n",
    "class FiLM(nn.Module):\n",
    "    def __init__(self, embed_dim: int, num_features: int):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(embed_dim, num_features * 2)  # produce gamma and beta\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, emb: torch.Tensor):\n",
    "        \"\"\"\n",
    "        x: (B, C, H, W)\n",
    "        emb: (B, embed_dim)\n",
    "        \"\"\"\n",
    "        params = self.fc(emb)  # (B, 2*C)\n",
    "        gamma, beta = params.chunk(2, dim=1)\n",
    "        gamma = gamma.unsqueeze(-1).unsqueeze(-1)\n",
    "        beta = beta.unsqueeze(-1).unsqueeze(-1)\n",
    "        return x * (1 + gamma) + beta\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e932d6a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------\n",
    "# U-Net style Encoder\n",
    "# --------------------------------------\n",
    "class UNetEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    U-Net encoder that compresses image into features.\n",
    "    Does NOT receive label in this variant (we keep encoder label-free for better swap controllability).\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=1, base_channels=32, num_down=4):\n",
    "        super().__init__()\n",
    "        ch = base_channels\n",
    "        self.initial = conv_block(in_channels, ch)\n",
    "\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.skip_channels = [ch]  # ✅ record initial skip channel\n",
    "\n",
    "        for i in range(num_down):\n",
    "            self.downs.append(nn.Sequential(\n",
    "                conv_block(ch, ch*2, kernel=4, stride=2, padding=1),  # downsample\n",
    "            ))\n",
    "            ch *= 2\n",
    "            self.skip_channels.append(ch)  # ✅ record skip channel after each down\n",
    "\n",
    "        # final conv to map to latent intermediate feature map\n",
    "        self.final = conv_block(ch, ch)\n",
    "\n",
    "        self.out_channels = ch\n",
    "\n",
    "    def forward(self, x):\n",
    "        skips = []\n",
    "        x = self.initial(x)\n",
    "        skips.append(x)\n",
    "        for d in self.downs:\n",
    "            x = d(x)\n",
    "            skips.append(x)\n",
    "        x = self.final(x)\n",
    "        return x, skips  # x is deepest feature map, skips for decoder\n",
    "    \n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9326ac0d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------\n",
    "# Latent projection: map feature map -> latent vector (mu, logvar)\n",
    "# and back (latent -> feature map)\n",
    "# --------------------------------------\n",
    "class LatentMapper(nn.Module):\n",
    "    def __init__(self, feat_channels, latent_dim=256):\n",
    "        super().__init__()\n",
    "        self.feat_channels = feat_channels\n",
    "        self.latent_dim = latent_dim\n",
    "        # global pooling -> FC -> mu/logvar\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc_mu = nn.Linear(feat_channels, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(feat_channels, latent_dim)\n",
    "\n",
    "        # map latent back to channels spatial map (up-projection)\n",
    "        self.fc_dec = nn.Linear(latent_dim, feat_channels)\n",
    "\n",
    "    def encode(self, feat):\n",
    "        # feat: (B, C, H, W)\n",
    "        b, c, _, _ = feat.shape\n",
    "        pooled = self.pool(feat).view(b, c)  # (B, C)\n",
    "        mu = self.fc_mu(pooled)\n",
    "        logvar = self.fc_logvar(pooled)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = (0.5 * logvar).exp()\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode_latent_to_feat(self, z):\n",
    "        # z: (B, latent_dim)\n",
    "        x = self.fc_dec(z)  # (B, feat_channels)\n",
    "        # reshape to (B, C, 1, 1) then broadcast spatially when needed\n",
    "        return x.unsqueeze(-1).unsqueeze(-1)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11088eb8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------\n",
    "# U-Net Decoder conditioned on label via FiLM and concatenation\n",
    "# --------------------------------------\n",
    "class UNetDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder: takes latent features and label embedding to produce image.\n",
    "    Uses skip connections and FiLM modulations at each decoding block.\n",
    "    Handles dynamic input channels from encoder skips.\n",
    "    \"\"\"\n",
    "    def __init__(self, out_channels=1, base_channels=32, num_up=4, latent_feat_channels=512, \n",
    "                 label_embed_dim=32, encoder_channels: list = None):\n",
    "        super().__init__()\n",
    "        assert encoder_channels is not None, \"You must pass encoder skip channels\"\n",
    "        self.num_up = num_up\n",
    "        self.encoder_channels = encoder_channels[::-1]  # reverse to match decoder order\n",
    "\n",
    "        # label embedding\n",
    "        self.label_embed = nn.Sequential(\n",
    "            nn.Linear(1, label_embed_dim),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.up_convs = nn.ModuleList()\n",
    "        self.films = nn.ModuleList()\n",
    "        ch = latent_feat_channels\n",
    "\n",
    "        # dynamically set input channels based on skip connections\n",
    "        for i in range(num_up):\n",
    "            skip_ch = self.encoder_channels[i+1] if i+1 < len(self.encoder_channels) else 0\n",
    "            in_ch = ch + skip_ch   # <--- include skip channels here\n",
    "            out_ch = ch // 2\n",
    "            self.up_convs.append(upconv_block(in_ch, out_ch))\n",
    "            self.films.append(FiLM(label_embed_dim, out_ch))\n",
    "            ch = out_ch\n",
    "\n",
    "        # final conv: expects concatenated channels from last upconv + first skip\n",
    "        final_in_ch = ch + self.encoder_channels[0]\n",
    "        self.final_conv = nn.Sequential(\n",
    "            conv_block(final_in_ch, ch),\n",
    "            nn.Conv2d(ch, out_channels, kernel_size=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, feat_from_latent, skips, label):\n",
    "        B = feat_from_latent.shape[0]\n",
    "        x = feat_from_latent\n",
    "        emb = self.label_embed(label.view(B, -1))\n",
    "        skips_rev = skips[::-1]\n",
    "\n",
    "        for i, (up, film) in enumerate(zip(self.up_convs, self.films)):\n",
    "            if i+1 < len(skips_rev):\n",
    "                skip = skips_rev[i+1]\n",
    "                if x.shape[-2:] != skip.shape[-2:]:\n",
    "                    x = F.interpolate(x, size=skip.shape[-2:], mode='bilinear', align_corners=False)\n",
    "                x = torch.cat([x, skip], dim=1)   # concat first\n",
    "                x = up(x)\n",
    "                x = film(x, emb)\n",
    "            else:\n",
    "                x = up(x)\n",
    "                x = film(x, emb)\n",
    "\n",
    "        if x.shape[2:] != skips_rev[0].shape[2:]:\n",
    "            skips_rev[0] = F.interpolate(skips_rev[0], size=x.shape[2:], mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "        out = self.final_conv(torch.cat([x, skips_rev[0]], dim=1))\n",
    "        return out\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c28e72c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------\n",
    "# Conditional VAE model (Encoder + LatentMapper + Decoder)\n",
    "# --------------------------------------\n",
    "class ConditionalVAE_UNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 img_channels=1,\n",
    "                 base_channels=32,\n",
    "                 num_scales=4,\n",
    "                 latent_dim=256,\n",
    "                 label_embed_dim=32,\n",
    "                 device: torch.device = torch.device('cpu')):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.encoder = UNetEncoder(in_channels=img_channels, base_channels=base_channels, num_down=num_scales)\n",
    "        feat_ch = self.encoder.out_channels\n",
    "        self.latent_map = LatentMapper(feat_ch, latent_dim=latent_dim)\n",
    "        # the decoder expects the latent mapped to same count of channels as feat_ch\n",
    "        self.decoder = UNetDecoder(out_channels=img_channels,\n",
    "                                   base_channels=base_channels,\n",
    "                                   num_up=num_scales,\n",
    "                                   latent_feat_channels=feat_ch,\n",
    "                                   label_embed_dim=label_embed_dim,\n",
    "                                    encoder_channels=self.encoder.skip_channels)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, label: torch.Tensor, sample: bool = True):\n",
    "        \"\"\"\n",
    "        Forward pass for reconstruction training.\n",
    "        x: (B, C, H, W)\n",
    "        label: (B,1) float tensor (0 or 1) or continuous\n",
    "        returns: reconstructed image, mu, logvar, z\n",
    "        \"\"\"\n",
    "        feat, skips = self.encoder(x)  # feat: deepest feature map\n",
    "        mu, logvar = self.latent_map.encode(feat)\n",
    "        if sample:\n",
    "            z = self.latent_map.reparameterize(mu, logvar)\n",
    "        else:\n",
    "            z = mu\n",
    "        # project latent back to feature map\n",
    "        feat_z = self.latent_map.decode_latent_to_feat(z)  # (B, featC, 1, 1)\n",
    "        # decode with label\n",
    "        x_recon = self.decoder(feat_z, skips, label)\n",
    "        return x_recon, mu, logvar, z\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "052b246a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --------------------------------------\n",
    "# PatchGAN Discriminator (conditional)\n",
    "# --------------------------------------\n",
    "class PatchDiscriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    PatchGAN discriminator that optionally conditions on label via concatenation/projection.\n",
    "    Output is patch map of real/fake logits.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=1, base_channels=32, n_layers=4, label_condition=True):\n",
    "        super().__init__()\n",
    "        self.label_condition = label_condition\n",
    "        ch = base_channels\n",
    "        layers = [nn.Conv2d(in_channels + (1 if label_condition else 0), ch, kernel_size=4, stride=2, padding=1),\n",
    "                  nn.LeakyReLU(0.2, inplace=True)]\n",
    "        for i in range(1, n_layers):\n",
    "            layers += [spectral_norm(nn.Conv2d(ch, ch*2, kernel_size=4, stride=2, padding=1)), nn.LeakyReLU(0.2, inplace=True)]\n",
    "            ch *= 2\n",
    "        # final conv to produce 1-channel patch score\n",
    "        layers += [nn.Conv2d(ch, 1, kernel_size=4, stride=1, padding=1)]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, img: torch.Tensor, label: torch.Tensor = None):\n",
    "        # if conditional, concat label as extra channel broadcasted spatially\n",
    "        if self.label_condition:\n",
    "            assert label is not None\n",
    "            B, _, H, W = img.shape\n",
    "            # label shape (B,1)\n",
    "            lab_map = label.view(B, 1, 1, 1).expand(-1, 1, H, W)\n",
    "            x = torch.cat([img, lab_map], dim=1)\n",
    "        else:\n",
    "            x = img\n",
    "        return self.model(x)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1b80e49",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --------------------------------------\n",
    "# Perceptual feature extractor and loss\n",
    "# Prefers torchxrayvision DenseNet if available; else uses VGG16 features\n",
    "# --------------------------------------\n",
    "class PerceptualFeatureExtractor(nn.Module):\n",
    "    def __init__(self, device: torch.device, layers: List[int] = [3, 8, 15, 22], prefer_txrv=True):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.preferred = False\n",
    "        self.layers = layers\n",
    "        if prefer_txrv and _HAS_TXRV:\n",
    "            # Use torchxrayvision DenseNet pretrained on ChestX-ray14 (or CheXpert) if available\n",
    "            # NOTE: model architecture selection may need to be adapted by the user\n",
    "            # Here we choose DenseNet121 as an example\n",
    "            self.net = xrv.models.DenseNet(weights=\"densenet121-res224-all\").to(device).eval()\n",
    "            for p in self.net.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "            # torchxrayvision expects normalized input differently; we'll not freeze transformation here\n",
    "            self.preferred = True\n",
    "            # For simplicity, we'll use the raw features from penultimate layer. Expose forward hook\n",
    "        else:\n",
    "            # Fallback: use VGG16 features from torchvision\n",
    "            vgg = models.vgg16(pretrained=True).features.to(device).eval()\n",
    "            self.net = vgg[:max(layers) + 1]\n",
    "            for p in self.net.parameters():\n",
    "                p.requires_grad = False\n",
    "            self.preferred = False\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> List[torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Return list of feature maps from selected layers.\n",
    "        Note: x must be normalized appropriately by caller (ImageNet norm for VGG).\n",
    "        For torchxrayvision models, x should be in same normalization as model expects (refer to torchxrayvision docs).\n",
    "        \"\"\"\n",
    "        if self.preferred and _HAS_TXRV:\n",
    "            # torchxrayvision DenseNet121: call full model and extract intermediate features if required.\n",
    "            # The object returned by xrv DenseNet may already provide embeddings/predictions.\n",
    "            # For compatibility we return the penultimate feature map repeated.\n",
    "            with torch.no_grad():\n",
    "                # model.forward: returns logits and embedding depending on model configuration\n",
    "                # Use model.features if available, else model.forward gives preds only.\n",
    "                try:\n",
    "                    # attempt to use model.features (if available)\n",
    "                    feats = self.net.features(x)\n",
    "                    # choose some mid layers - here we'll take the last few feature maps as proxies\n",
    "                    return [feats]\n",
    "                except Exception:\n",
    "                    # fallback: return output embedding\n",
    "                    emb = self.net(x)\n",
    "                    return [emb]\n",
    "        else:\n",
    "            # VGG path: step through layers and collect\n",
    "            features = []\n",
    "            curr = x\n",
    "            for i, layer in enumerate(self.net):\n",
    "                curr = layer(curr)\n",
    "                if i in self.layers:\n",
    "                    features.append(curr)\n",
    "            return features\n",
    "\n",
    "\n",
    "class PerceptualLoss(nn.Module):\n",
    "    def __init__(self, device: torch.device, layers: List[int] = [3, 8, 15, 22], pref_txrv=True):\n",
    "        super().__init__()\n",
    "        self.fe = PerceptualFeatureExtractor(device, layers=layers, prefer_txrv=pref_txrv)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def forward(self, gen: torch.Tensor, tgt: torch.Tensor) -> torch.Tensor:\n",
    "        # gen and tgt must be pre-normalized to match feature extractor expectations\n",
    "        if not self.fe.preferred:  # VGG path\n",
    "            norm = transforms.Normalize(mean=[0.485,0.456,0.406],\n",
    "                                        std=[0.229,0.224,0.225])\n",
    "            gen = torch.stack([norm(img) for img in gen])\n",
    "            tgt = torch.stack([norm(img) for img in tgt])\n",
    "        gf = self.fe(gen)\n",
    "        tf = self.fe(tgt)\n",
    "        loss = 0.0\n",
    "        for a, b in zip(gf, tf):\n",
    "            loss = loss + self.criterion(a, b)\n",
    "        return loss\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6568655",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------\n",
    "# Loss helpers for VAE\n",
    "# --------------------------------------\n",
    "def kl_divergence(mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n",
    "    # returns sum over latent dims per batch, averaged by batch\n",
    "    # standard VAE KL between q ~ N(mu, sigma^2) and p ~ N(0,I)\n",
    "    kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1)\n",
    "    return torch.mean(kld)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7bbed046",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------\n",
    "# SSIM utilities\n",
    "# --------------------------------------\n",
    "def compute_ssim_pair(img_a: np.ndarray, img_b: np.ndarray, multichannel=False) -> float:\n",
    "    \"\"\"\n",
    "    Compute SSIM for a single image pair.\n",
    "    img_a, img_b: numpy arrays in [0,1] or [0,255], dtype float. If using sk_ssim ensure range argument set appropriately.\n",
    "    \"\"\"\n",
    "    # If grayscale, sk_ssim expects 2D arrays\n",
    "    if img_a.ndim == 3 and img_a.shape[2] == 1:\n",
    "        img_a = img_a.squeeze(2)\n",
    "        img_b = img_b.squeeze(2)\n",
    "    # Use skimage ssim; ensure float in [0,1]\n",
    "    return sk_ssim(img_a, img_b, data_range=img_a.max() - img_a.min())\n",
    "\n",
    "\n",
    "def compute_ssim_batch_numpy(origs: np.ndarray, gens: np.ndarray, labels: Optional[np.ndarray] = None):\n",
    "    \"\"\"\n",
    "    origs, gens: arrays of shape (N,H,W) or (N,H,W,C)\n",
    "    labels: optional (N,) with 0/1 indicating normal/pneumonia\n",
    "    returns dict with full list, normal list, pneumonia list\n",
    "    \"\"\"\n",
    "    N = origs.shape[0]\n",
    "    all_scores = []\n",
    "    normal_scores = []\n",
    "    pneu_scores = []\n",
    "    for i in range(N):\n",
    "        s = compute_ssim_pair(origs[i], gens[i])\n",
    "        all_scores.append(s)\n",
    "        if labels is not None:\n",
    "            if labels[i] == 0:\n",
    "                normal_scores.append(s)\n",
    "            else:\n",
    "                pneu_scores.append(s)\n",
    "    result = {\n",
    "        \"all\": np.array(all_scores),\n",
    "        \"normal\": np.array(normal_scores),\n",
    "        \"pneumonia\": np.array(pneu_scores)\n",
    "    }\n",
    "    return result\n",
    "\n",
    "\n",
    "def plot_ssim_distributions(ssim_dict: Dict[str, np.ndarray], bins=50, figsize=(12, 4)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.hist(ssim_dict['all'], bins=bins)\n",
    "    plt.title('SSIM - All')\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.hist(ssim_dict['normal'], bins=bins)\n",
    "    plt.title('SSIM - Normal')\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.hist(ssim_dict['pneumonia'], bins=bins)\n",
    "    plt.title('SSIM - Pneumonia')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91a64a32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --------------------------------------\n",
    "# Utility: load a pretrained pneumonia classifier\n",
    "# Prefer torchxrayvision DenseNet finetuned; else user supplies path\n",
    "# --------------------------------------\n",
    "def load_pretrained_pneumonia_classifier(device: torch.device, model_name: str = \"densenet121\", checkpoint_path: Optional[str] = None):\n",
    "    \"\"\"\n",
    "    Try to load a pneumonia classifier:\n",
    "     - If torchxrayvision available, load DenseNet pretrained on CXR data.\n",
    "     - Else, if checkpoint_path provided, load a torch checkpoint into ResNet/DenseNet skeleton.\n",
    "    Returns a model in eval() on device and a preprocessing function (callable) that converts raw images to model input.\n",
    "    \"\"\"\n",
    "    if _HAS_TXRV:\n",
    "        # Example: DenseNet trained on combined datasets. xrv wrapper gives standardized preprocess.\n",
    "        model = xrv.models.DenseNet(weights=\"densenet121-res224-all\").to(device)\n",
    "        model.eval()\n",
    "\n",
    "        def preprocess(x: torch.Tensor):\n",
    "            # torchxrayvision models usually expect: (B,1,H,W) normalized to range and standardization\n",
    "            # xrv has own normalization; use xrv.utils.normalize? Here assume input in [0,1]\n",
    "            # Use xrv.utils.normalize in actual usage; for now return x\n",
    "            return x\n",
    "\n",
    "        return model, preprocess\n",
    "    else:\n",
    "        # Fallback: user must supply checkpoint_path. We'll create ResNet18 skeleton as example.\n",
    "        if checkpoint_path is None:\n",
    "            raise RuntimeError(\"No torchxrayvision available and no checkpoint_path provided. Please provide a classifier checkpoint.\")\n",
    "        # Example skeleton: ResNet18 with single channel input adapted\n",
    "        model = torchvision.models.resnet18(pretrained=False)\n",
    "        # modify first conv to accept 1 channel\n",
    "        model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        # modify final fc to binary\n",
    "        model.fc = nn.Linear(model.fc.in_features, 2)\n",
    "        ckpt = torch.load(checkpoint_path, map_location=device)\n",
    "        model.load_state_dict(ckpt)\n",
    "        model = model.to(device).eval()\n",
    "\n",
    "        def preprocess(x: torch.Tensor):\n",
    "            return x  # user should implement the required normalization\n",
    "\n",
    "        return model, preprocess\n",
    "\n",
    "\n",
    "def classify_batch(model, preprocess_fn, imgs: torch.Tensor, device: torch.device):\n",
    "    \"\"\"\n",
    "    imgs: Tensor (B,1,H,W) in raw float range expected by preprocess_fn\n",
    "    returns predicted labels (B,) (0 or 1) and softmax probs\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    inp = preprocess_fn(imgs.to(device))\n",
    "    with torch.no_grad():\n",
    "        logits = model(inp)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        preds = torch.argmax(probs, dim=1).cpu().numpy()\n",
    "        scores = probs[:, 1].cpu().numpy()  # pneumonia probability if label 1 is pneumonia\n",
    "    return preds, scores\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ecad80a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --------------------------------------\n",
    "# Trainer skeleton: single-step functions for discriminator & generator update calculations\n",
    "# (Does not include data loader loops)\n",
    "# --------------------------------------\n",
    "class CVAEGANTrainer:\n",
    "    def __init__(self,\n",
    "                 device: torch.device,\n",
    "                 model: ConditionalVAE_UNet,\n",
    "                 discriminator: PatchDiscriminator,\n",
    "                 perceptual_loss_fn: PerceptualLoss,\n",
    "                 lr_g: float = 2e-4,\n",
    "                 lr_d: float = 2e-4,\n",
    "                 lambda_adv: float = 0.5,\n",
    "                 lambda_perc: float = 1.0,\n",
    "                 lambda_l1: float = 10.0,\n",
    "                 lambda_kl: float = 1.0):\n",
    "        self.device = device\n",
    "        self.model = model.to(device)\n",
    "        self.discriminator = discriminator.to(device)\n",
    "        self.perc = perceptual_loss_fn\n",
    "        self.lambda_adv = lambda_adv\n",
    "        self.lambda_perc = lambda_perc\n",
    "        self.lambda_l1 = lambda_l1\n",
    "        self.lambda_kl = lambda_kl\n",
    "\n",
    "        self.optim_g = torch.optim.Adam(self.model.parameters(), lr=lr_g, betas=(0.5, 0.999))\n",
    "        self.optim_d = torch.optim.Adam(self.discriminator.parameters(), lr=lr_d, betas=(0.5, 0.999))\n",
    "        # adversarial criterion\n",
    "        self.adv_loss_fn = nn.BCEWithLogitsLoss()\n",
    "        # pixel loss\n",
    "        self.l1 = nn.L1Loss()\n",
    "\n",
    "    def train_step(self, imgs: torch.Tensor, labels: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Single training step:\n",
    "         - imgs: (B,1,H,W) normalized to [-1,1] (decoder uses Tanh output)\n",
    "         - labels: (B,1) float (0 or 1)\n",
    "        Returns dict of losses\n",
    "        \"\"\"\n",
    "        B = imgs.shape[0]\n",
    "        imgs = imgs.to(self.device)\n",
    "        labels = labels.to(self.device)\n",
    "\n",
    "        # ----------------------------\n",
    "        # Forward generator (VAE)\n",
    "        # ----------------------------\n",
    "        self.model.train()\n",
    "        recon_imgs, mu, logvar, z = self.model(imgs, labels, sample=True)\n",
    "\n",
    "        # ----------------------------\n",
    "        # Update discriminator\n",
    "        # ----------------------------\n",
    "        self.discriminator.train()\n",
    "        # Real: label 1 for real\n",
    "        real_logits = self.discriminator(imgs, labels)\n",
    "        real_targets = torch.ones_like(real_logits).to(self.device)\n",
    "        d_loss_real = self.adv_loss_fn(real_logits, real_targets)\n",
    "\n",
    "        # Fake: feed generated images (detach so gradients not flow to generator)\n",
    "        fake_logits = self.discriminator(recon_imgs.detach(), labels)\n",
    "        fake_targets = torch.zeros_like(fake_logits).to(self.device)\n",
    "        d_loss_fake = self.adv_loss_fn(fake_logits, fake_targets)\n",
    "\n",
    "        d_loss = (d_loss_real + d_loss_fake) * 0.5\n",
    "        self.optim_d.zero_grad()\n",
    "        d_loss.backward()\n",
    "        self.optim_d.step()\n",
    "\n",
    "        # ----------------------------\n",
    "        # Update generator (VAE encoder+decoder)\n",
    "        # ----------------------------\n",
    "        # compute adversarial loss with labels as want D to predict real\n",
    "        adv_logits = self.discriminator(recon_imgs, labels)\n",
    "        adv_targets = torch.ones_like(adv_logits).to(self.device)\n",
    "        adv_loss = self.adv_loss_fn(adv_logits, adv_targets)\n",
    "\n",
    "        # pixel reconstruction (L1) — imgs and recon_imgs expected [-1,1] range\n",
    "        recon_loss = self.l1(recon_imgs, imgs)\n",
    "\n",
    "        # perceptual loss - model expects inputs normalized appropriately depending on extractor\n",
    "        # NOTE: user must ensure normalization is consistent for perceptual extractor (ImageNet or CXR)\n",
    "        perc_loss = self.perc(recon_imgs, imgs)\n",
    "\n",
    "        # KL\n",
    "        kld = kl_divergence(mu, logvar)\n",
    "\n",
    "        g_loss = (self.lambda_l1 * recon_loss) + (self.lambda_kl * kld) + (self.lambda_adv * adv_loss) + (self.lambda_perc * perc_loss)\n",
    "\n",
    "        self.optim_g.zero_grad()\n",
    "        g_loss.backward()\n",
    "        self.optim_g.step()\n",
    "\n",
    "        return {\n",
    "            \"d_loss\": d_loss.item(),\n",
    "            \"g_loss\": g_loss.item(),\n",
    "            \"recon_loss\": recon_loss.item(),\n",
    "            \"adv_loss\": adv_loss.item(),\n",
    "            \"perc_loss\": perc_loss.item(),\n",
    "            \"kl\": kld.item()\n",
    "        }\n",
    "\n",
    "    def generate_opposite_label(self, imgs: torch.Tensor, labels: torch.Tensor, deterministic=True):\n",
    "        \"\"\"\n",
    "        Given imgs (B,1,H,W) and labels (B,1), generate images with swapped label.\n",
    "        deterministic=True uses mu instead of sampling z for reproducible outputs.\n",
    "        \"\"\"\n",
    "        imgs = imgs.to(self.device)\n",
    "        labels = labels.to(self.device)\n",
    "        swapped = 1.0 - labels  # assumes binary 0/1\n",
    "        with torch.no_grad():\n",
    "            recon, mu, logvar, z = self.model(imgs, labels, sample=not deterministic)\n",
    "            # To generate opposite label, we should re-encode (using encoder path) but then send swapped label to decoder.\n",
    "            # Re-run encoder to get mu, but we already have mu above. Use mu for deterministic\n",
    "            if deterministic:\n",
    "                z_use = mu\n",
    "            else:\n",
    "                z_use = z\n",
    "            feat_z = self.model.latent_map.decode_latent_to_feat(z_use.to(self.device))\n",
    "\n",
    "            # Above is placeholder: easier to re-run encode->latent->decode flow with swapped label using model.forward (but model.forward expects input label used by encoder)\n",
    "            # We'll implement a dedicated function to perform decode given z and swapped label:\n",
    "            # Use latent mapping decode + feed decoder with encoder skips using encoder(imgs)\n",
    "            feat, skips = self.model.encoder(imgs)\n",
    "            mu2, logvar2 = self.model.latent_map.encode(feat)\n",
    "            if deterministic:\n",
    "                z2 = mu2\n",
    "            else:\n",
    "                z2 = self.model.latent_map.reparameterize(mu2, logvar2)\n",
    "            feat_z2 = self.model.latent_map.decode_latent_to_feat(z2)\n",
    "            # now decode with swapped label\n",
    "            gen_swap = self.model.decoder(feat_z2, skips, swapped)\n",
    "        return gen_swap  # (B,1,H,W) in [-1,1]\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68df2107",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --------------------------------------\n",
    "# Evaluation: generate opposite label images for a full dataset and compute confusion matrix using classifier\n",
    "# --------------------------------------\n",
    "def evaluate_translations_and_confusion(classifier_model, preprocess_fn, cvae_model: ConditionalVAE_UNet,\n",
    "                                         dataloader, device: torch.device, label_map: Dict[int, str] = {0: \"normal\", 1: \"pneumonia\"}):\n",
    "    \"\"\"\n",
    "    Given a dataloader that yields (img, label, optional meta),\n",
    "    generate swapped-label images for every example and run through the classifier, producing confusion matrix.\n",
    "    Returns:\n",
    "      - y_true: np.array (N,) original labels\n",
    "      - y_pred_on_swapped: np.array (N,) predictions of classifier on swapped images\n",
    "      - probs: np.array (N,) classifier probability for 'pneumonia' class on swapped images\n",
    "    \"\"\"\n",
    "    cvae_model.eval()\n",
    "    ys_true = []\n",
    "    ys_pred_swapped = []\n",
    "    probs_swapped = []\n",
    "    images_orig = []\n",
    "    images_swapped = []\n",
    "    for batch in dataloader:\n",
    "        # Expect batch to be (imgs, labels) or (imgs, labels, ...). The user should adapt if different.\n",
    "        imgs = batch[0].to(device)\n",
    "        labels = batch[1].to(device)\n",
    "        with torch.no_grad():\n",
    "            # encode -> decode with swapped label\n",
    "            feat, skips = cvae_model.encoder(imgs)\n",
    "            mu, logvar = cvae_model.latent_map.encode(feat)\n",
    "            z = mu  # deterministic\n",
    "            feat_z = cvae_model.latent_map.decode_latent_to_feat(z)\n",
    "            swapped = 1.0 - labels\n",
    "            gen_swapped = cvae_model.decoder(feat_z, skips, swapped)\n",
    "            # convert to input range expected by classifier\n",
    "            imgs_for_classifier = preprocess_fn(gen_swapped.cpu())\n",
    "            preds, probs = classify_batch(classifier_model, preprocess_fn, gen_swapped.cpu(), device)\n",
    "            ys_true.append(labels.cpu().numpy().ravel())\n",
    "            ys_pred_swapped.append(preds)\n",
    "            probs_swapped.append(probs)\n",
    "            images_orig.append(imgs.cpu().numpy())\n",
    "            images_swapped.append(gen_swapped.cpu().numpy())\n",
    "\n",
    "    y_true = np.concatenate(ys_true, axis=0)\n",
    "    y_pred = np.concatenate(ys_pred_swapped, axis=0)\n",
    "    probs = np.concatenate(probs_swapped, axis=0)\n",
    "    images_orig = np.concatenate(images_orig, axis=0)\n",
    "    images_swapped = np.concatenate(images_swapped, axis=0)\n",
    "\n",
    "    # confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[label_map[0], label_map[1]])\n",
    "    return {\n",
    "        \"y_true\": y_true,\n",
    "        \"y_pred_swapped\": y_pred,\n",
    "        \"probs_swapped\": probs,\n",
    "        \"confusion_matrix\": cm,\n",
    "        \"confusion_display\": disp,\n",
    "        \"images_orig\": images_orig,\n",
    "        \"images_swapped\": images_swapped\n",
    "    }\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0abc391d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --------------------------------------\n",
    "# SSIM evaluation wrapper\n",
    "# --------------------------------------\n",
    "def evaluate_self_reconstruction_ssim(cvae_model: ConditionalVAE_UNet, dataloader, device: torch.device):\n",
    "    \"\"\"\n",
    "    For each image in dataloader, reconstruct using same label and compute SSIM between original and reconstructed.\n",
    "    Returns numpy arrays of original images, reconstructed images, labels.\n",
    "    \"\"\"\n",
    "    cvae_model.eval()\n",
    "    origs = []\n",
    "    gens = []\n",
    "    labs = []\n",
    "    for batch in dataloader:\n",
    "        imgs = batch[0].to(device)\n",
    "        labels = batch[1].to(device)\n",
    "        with torch.no_grad():\n",
    "            recon, mu, logvar, z = cvae_model(imgs, labels, sample=False)\n",
    "        # move to cpu numpy and scale to [0,1]\n",
    "        origs.append(((imgs.cpu().numpy() + 1.0) / 2.0).squeeze(1))   # assumes imgs in [-1,1]\n",
    "        gens.append(((recon.cpu().numpy() + 1.0) / 2.0).squeeze(1))\n",
    "        labs.append(labels.cpu().numpy().ravel())\n",
    "    origs = np.concatenate(origs, axis=0)\n",
    "    gens = np.concatenate(gens, axis=0)\n",
    "    labs = np.concatenate(labs, axis=0)\n",
    "    ssim_dict = compute_ssim_batch_numpy(origs, gens, labels=labs)\n",
    "    return ssim_dict, origs, gens, labs\n",
    "\n",
    "\n",
    "# --------------------------------------\n",
    "# Example of how to plot confusion matrix (user calls .plot())\n",
    "# --------------------------------------\n",
    "def plot_confusion_matrix_from_eval(eval_res):\n",
    "    disp = eval_res[\"confusion_display\"]\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    disp.plot(ax=ax)\n",
    "    plt.title(\"Confusion Matrix for Classifier on Swapped-label Generated Images\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------------------------\n",
    "# End of module\n",
    "# -------------------------------------------------------------------------------------------\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8024226-01bf-4fb2-b8bc-9c8298de4b4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listing objects from gs://rsna-pneumonia-x-rays/chest_xray/chest_xray/train/ ...\n",
      "✅ Found 5216 samples under chest_xray/chest_xray/train/\n",
      "Listing objects from gs://rsna-pneumonia-x-rays/chest_xray/chest_xray/val/ ...\n",
      "✅ Found 16 samples under chest_xray/chest_xray/val/\n",
      "Listing objects from gs://rsna-pneumonia-x-rays/chest_xray/chest_xray/test/ ...\n",
      "✅ Found 624 samples under chest_xray/chest_xray/test/\n",
      "Train size: 5216\n",
      "Val size: 16\n",
      "Test size: 624\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from google.cloud import storage\n",
    "from PIL import Image\n",
    "import io\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# -------------------------------\n",
    "# Dataset class (Option B style with checks)\n",
    "# -------------------------------\n",
    "class GCSChestXrayDataset(Dataset):\n",
    "    def __init__(self, project_id, bucket_name, prefix, transform=None):\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "\n",
    "        client = storage.Client(project=project_id)\n",
    "        bucket = client.bucket(bucket_name)\n",
    "\n",
    "        print(f\"Listing objects from gs://{bucket_name}/{prefix} ...\")\n",
    "\n",
    "        blobs = bucket.list_blobs(prefix=prefix)\n",
    "        for blob in blobs:\n",
    "            if blob.name.endswith(\".jpeg\") or blob.name.endswith(\".jpg\") or blob.name.endswith(\".png\"):\n",
    "                # Infer label from folder name\n",
    "                if \"NORMAL\" in blob.name.upper():\n",
    "                    label = 0\n",
    "                elif \"PNEUMONIA\" in blob.name.upper():\n",
    "                    label = 1\n",
    "                else:\n",
    "                    continue\n",
    "                self.samples.append((blob.name, label))\n",
    "\n",
    "        print(f\"✅ Found {len(self.samples)} samples under {prefix}\")\n",
    "\n",
    "        self.bucket = bucket\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        blob_name, label = self.samples[idx]\n",
    "        blob = self.bucket.blob(blob_name)\n",
    "        img_bytes = blob.download_as_bytes()\n",
    "        img = Image.open(io.BytesIO(img_bytes)).convert(\"L\")\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "# -------------------------------\n",
    "# Transforms\n",
    "# -------------------------------\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# -------------------------------\n",
    "# Load datasets\n",
    "# -------------------------------\n",
    "project_id = \"pneumonia-generator\"\n",
    "bucket_name = \"rsna-pneumonia-x-rays\"\n",
    "\n",
    "train_dataset = GCSChestXrayDataset(project_id, bucket_name, prefix=\"chest_xray/chest_xray/train/\", transform=transform)\n",
    "val_dataset   = GCSChestXrayDataset(project_id, bucket_name, prefix=\"chest_xray/chest_xray/val/\", transform=transform)\n",
    "test_dataset  = GCSChestXrayDataset(project_id, bucket_name, prefix=\"chest_xray/chest_xray/test/\", transform=transform)\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Debug: print dataset lengths\n",
    "# -------------------------------\n",
    "print(\"Train size:\", len(train_dataset))\n",
    "print(\"Val size:\", len(val_dataset))\n",
    "print(\"Test size:\", len(test_dataset))\n",
    "\n",
    "# -------------------------------\n",
    "# Create loaders only if non-empty\n",
    "# -------------------------------\n",
    "def safe_loader(dataset, batch_size, shuffle):\n",
    "    if len(dataset) == 0:\n",
    "        print(\"⚠️ WARNING: Empty dataset, loader will be None\")\n",
    "        return None\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=2)\n",
    "\n",
    "train_loader = safe_loader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader   = safe_loader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader  = safe_loader(test_dataset, batch_size=32, shuffle=False)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a42fcb8-3fb6-450d-9d95-234073cb1b80",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training...\n",
      "Epoch 1/5\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Instantiate model, discriminator, perceptual loss\n",
    "cvae_model = ConditionalVAE_UNet(device=device)\n",
    "discriminator = PatchDiscriminator()\n",
    "perc_loss_fn = PerceptualLoss(device=device)\n",
    "\n",
    "trainer = CVAEGANTrainer(device, cvae_model, discriminator, perc_loss_fn)\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "print(\"Starting Training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    epoch_losses = {\"d_loss\": [], \"g_loss\": [], \"recon_loss\": [], \"adv_loss\": [], \"perc_loss\": [], \"kl\": []}\n",
    "    \n",
    "    cvae_model.train()\n",
    "    for imgs, labels in train_loader:\n",
    "        labels = labels.unsqueeze(1).float()  # ensure (B,1) float\n",
    "        loss_dict = trainer.train_step(imgs, labels)\n",
    "        for k, v in loss_dict.items():\n",
    "            epoch_losses[k].append(v)\n",
    "    \n",
    "    # Print epoch averages\n",
    "    avg_losses = {k: np.mean(v) for k, v in epoch_losses.items()}\n",
    "    print(f\"Avg losses: {avg_losses}\")\n",
    "    \n",
    "print(\"Training Done\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92bbd5f-3ac0-4c1d-a6c2-826f00a23930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2a. SSIM self-reconstruction\n",
    "ssim_res, origs, gens, labs = evaluate_self_reconstruction_ssim(cvae_model, val_loader, device)\n",
    "print(\"SSIM Results:\")\n",
    "for k, v in ssim_res.items():\n",
    "    print(f\"{k}: mean={np.mean(v):.4f}, std={np.std(v):.4f}\")\n",
    "\n",
    "# 2b. Classifier-based swapped label evaluation\n",
    "classifier_model, preprocess_fn = load_pretrained_pneumonia_classifier(device)\n",
    "eval_res = evaluate_translations_and_confusion(classifier_model, preprocess_fn, cvae_model, val_loader, device)\n",
    "plot_confusion_matrix_from_eval(eval_res)\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m132",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m132"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
